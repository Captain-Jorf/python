import cv2
import dlib
import numpy as np
import pyautogui

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡
EYE_POINTS_LEFT = list(range(36, 42))  # Ù†Ù‚Ø§Ø· Ú†Ø´Ù… Ú†Ù¾
MOUTH_OPEN_THRESHOLD = 8               # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ø² Ø¨ÙˆØ¯Ù† Ø¯Ù‡Ø§Ù†
SMOOTHING_WINDOW = 5                   # Ù†Ø±Ù… Ú©Ø±Ø¯Ù† Ø­Ø±Ú©Øª
ACTIVE_ZONE_RATIO = 0.4                # Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„
ZONE_STEP = 0.05                        # Ú¯Ø§Ù… ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„
MIN_ZONE_RATIO = 0.1
MAX_ZONE_RATIO = 0.9

# ØµÙ Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù… Ú©Ø±Ø¯Ù† Ø­Ø±Ú©Øª
position_history = []

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

# Ø¯ÙˆØ±Ø¨ÛŒÙ†
cap = cv2.VideoCapture(0)

# Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú©Ù„ÛŒÚ© Ù…Ø¯Ø§ÙˆÙ…
mouse_pressed = False

print("ğŸ¥ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø´Ø±ÙˆØ¹ Ø´Ø¯.")
print("â•â– Ø¨Ø±Ø§ÛŒ ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„ Ø§Ø² Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ '+' Ùˆ '-' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")
print("ğŸ–±ï¸ Ø¨Ø±Ø§ÛŒ Ú©Ù„ÛŒÚ©ØŒ Ø¯Ù‡Ø§Ù†ØªØ§Ù† Ø±Ø§ Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯.")
print("âŒ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯.")

def get_eye_center(eye_points):
    x_coords = [p.x for p in eye_points]
    y_coords = [p.y for p in eye_points]
    center_x = int(np.mean(x_coords))
    center_y = int(np.mean(y_coords))
    return center_x, center_y

def is_mouth_open(landmarks):
    upper_lip = landmarks.part(62).y
    lower_lip = landmarks.part(66).y
    vertical_distance = abs(lower_lip - upper_lip)
    return vertical_distance > MOUTH_OPEN_THRESHOLD

while True:
    ret, frame = cap.read()
    if not ret:
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)

    for face in faces:
        landmarks = predictor(gray, face)

        # Ú¯Ø±ÙØªÙ† Ù†Ù‚Ø§Ø· Ú†Ø´Ù… Ú†Ù¾
        left_eye_points = [landmarks.part(i) for i in EYE_POINTS_LEFT]

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø±Ú©Ø² Ú†Ø´Ù… Ú†Ù¾
        eye_center_x, eye_center_y = get_eye_center(left_eye_points)

        # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚Ø¨Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù… Ú©Ø±Ø¯Ù† Ø­Ø±Ú©Øª
        position_history.append((eye_center_x, eye_center_y))
        if len(position_history) > SMOOTHING_WINDOW:
            position_history.pop(0)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù… Ú©Ø±Ø¯Ù† Ø­Ø±Ú©Øª
        avg_pos = np.mean(position_history, axis=0)
        current_eye = avg_pos.astype(int)

        # Ø±Ø³Ù… Ø¯Ø§ÛŒØ±Ù‡ Ø±ÙˆÛŒ Ú†Ø´Ù…
        cv2.circle(frame, (current_eye[0], current_eye[1]), 5, (0, 255, 0), -1)

        # ØªØ¹ÛŒÛŒÙ† Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„ (Active Zone)
        height, width = frame.shape[:2]
        screen_width, screen_height = pyautogui.size()

        active_zone_w = int(width * ACTIVE_ZONE_RATIO)
        active_zone_h = int(height * ACTIVE_ZONE_RATIO)

        center_x = width // 2
        center_y = height // 2

        x_start = center_x - active_zone_w // 2
        y_start = center_y - active_zone_h // 2
        x_end = x_start + active_zone_w
        y_end = y_start + active_zone_h

        # Ø±Ø³Ù… Ù…Ø³ØªØ·ÛŒÙ„ Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„
        cv2.rectangle(frame, (x_start, y_start), (x_end, y_end), (255, 255, 0), 1)

        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª Ú†Ø´Ù… Ø¨Ù‡ Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„
        rel_x = np.clip(current_eye[0], x_start, x_end)
        rel_y = np.clip(current_eye[1], y_start, y_end)

        # Ù†Ú¯Ø§Ø´Øª Ø¨Ù‡ ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´
        x_ratio = (rel_x - x_start) / active_zone_w
        y_ratio = (rel_y - y_start) / active_zone_h

        target_x = int(screen_width * (1 - x_ratio))  # Ø¨Ø±Ø¹Ú©Ø³ Ú©Ø±Ø¯Ù† Ú†Ù¾/Ø±Ø§Ø³Øª
        target_y = int(screen_height * y_ratio)

        # Ø­Ø±Ú©Øª Ù…ÙˆØ³ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø·Ù„Ù‚
        pyautogui.moveTo(target_x, target_y)

        # ØªØ´Ø®ÛŒØµ Ø¨Ø§Ø² Ø¨ÙˆØ¯Ù† Ø¯Ù‡Ø§Ù† Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù„ÛŒÚ©
        if is_mouth_open(landmarks):
            if not mouse_pressed:
                pyautogui.mouseDown()
                mouse_pressed = True
                print("ğŸ–±ï¸ Ú©Ù„ÛŒÚ© ÙØ¹Ø§Ù„ Ø´Ø¯!")
        else:
            if mouse_pressed:
                pyautogui.mouseUp()
                mouse_pressed = False
                print("MouseClicked!")

    # Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±
    cv2.imshow("Eye Controlled Mouse - Accessibility Mode", frame)

    # Ú©Ù†ØªØ±Ù„ Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
    key = cv2.waitKey(1)
    if key == ord('q'):
        break
    elif key == ord('+') or key == ord('='):
        ACTIVE_ZONE_RATIO = min(MAX_ZONE_RATIO, ACTIVE_ZONE_RATIO + ZONE_STEP)
        print(f"ğŸŸ© Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„: {int(ACTIVE_ZONE_RATIO * 100)}%")
    elif key == ord('-') or key == ord('_'):
        ACTIVE_ZONE_RATIO = max(MIN_ZONE_RATIO, ACTIVE_ZONE_RATIO - ZONE_STEP)
        print(f"ğŸŸ¥ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ù†Ø·Ù‚Ù‡ ÙØ¹Ø§Ù„: {int(ACTIVE_ZONE_RATIO * 100)}%")

# Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
if mouse_pressed:
    pyautogui.mouseUp()

cap.release()
cv2.destroyAllWindows()